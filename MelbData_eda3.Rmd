---
title: 'Melbourne Myki - Update: The fast & the curious'
date: '`r Sys.Date()`'
output:
  html_document:
    number_sections: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: cosmo
    highlight: tango
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo=TRUE, error=FALSE)
```

# Introduction

The aim of this project is to explore the integration between melbourne myki and automobile traffic.

**In this notebook**, we will first study and visualise the original data, engineer new features, and examine potential outliers. Then we add two **external data sets** on the weather and on the busiest routes. We visualise and analyse the new features within these data sets and their impact on the target *trip\_duration* values. Finally, we will make a brief excursion into viewing this challenge as a **classification problem** and finish this notebook with a **simple XGBoost model** that provides a basic prediction.


## Load libraries and helper functions

```{r, message = FALSE}
library('ggplot2') # visualisation
library('scales') # visualisation
library('grid') # visualisation
library('RColorBrewer') # visualisation
library('corrplot') # visualisation
library('alluvial') # visualisation
library('dplyr') # data manipulation
library('readr') # input/output
library('data.table') # data manipulation
library('tibble') # data wrangling
library('tidyr') # data wrangling
library('stringr') # string manipulation
library('forcats') # factor manipulation
library('lubridate') # date and time
library('geosphere') # geospatial locations
library('leaflet') # maps
library('leaflet.extras') # maps
library('maps') # maps
library('xgboost') # modelling
library('caret') # modelling
```

We use the *multiplot* function, courtesy of [R Cookbooks](http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/) to create multi-panel plots.

```{r}

# Define multiple plot function

#

# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)

# - cols:   Number of columns in layout

# - layout: A matrix specifying the layout. If present, 'cols' is ignored.

#

# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),

# then plot 1 will go in the upper left, 2 will go in the upper right, and

# 3 will go all the way across the bottom.

#

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {



  # Make a list from the ... arguments and plotlist

  plots <- c(list(...), plotlist)



  numPlots = length(plots)



  # If layout is NULL, then use 'cols' to determine layout

  if (is.null(layout)) {

    # Make the panel

    # ncol: Number of columns of plots

    # nrow: Number of rows needed, calculated from # of cols

    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),

                    ncol = cols, nrow = ceiling(numPlots/cols))

  }



 if (numPlots==1) {

    print(plots[[1]])



  } else {

    # Set up the page

    grid.newpage()

    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))



    # Make each plot, in the correct location

    for (i in 1:numPlots) {

      # Get the i,j matrix positions of the regions that contain this subplot

      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))



      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,

                                      layout.pos.col = matchidx$col))

    }

  }

}

```





## Load data



We use *data.table's* fread function to speed up reading in the data:



```{r warning=FALSE, results=FALSE}

setwd('~/MelbDatathon2018/')

load(file='all_calendar.RData')
load(file='all_card.RData')
load(file='all_stop_locations.RData')

ScanOnFolderMaster <- '~/MelbDatathon2018/Samp_0/ScanOnTransaction'
ScanOffFolderMaster <- '~/MelbDatathon2018/Samp_0/ScanOffTransaction'

mySamp <- 0

ScanOnFolder <- sub("x",mySamp,ScanOnFolderMaster)
ScanOffFolder <- sub("x",mySamp,ScanOffFolderMaster)

#list the files
onFiles <- list.files(ScanOnFolder,recursive = TRUE,full.names = TRUE)
offFiles <- list.files(ScanOffFolder,recursive = TRUE,full.names = TRUE)

#how many
allFiles <- union(onFiles,offFiles)
cat("\nthere are", length(allFiles),'files')

#dt <- fread(cmd,nrow=10000)


scanOntrain <- as.tibble(fread(paste0("gzip -dc ", onFiles[1]),nrow=10000))
scanOfftrain <- as.tibble(fread(paste0("gzip -dc ", offFiles[1]),nrow=10000))

colnames(scanOntrain) <- c('Mode','BusinessDate','DateTime','CardID','CardType','VehicleID','ParentRoute','RouteID','StopID')
colnames(scanOfftrain) <- c('Mode','BusinessDate','DateTime','CardID','CardType','VehicleID','ParentRoute','RouteID','StopID')

testOn <- as.tibble(fread(paste0("gzip -dc ", onFiles[1]),nrow=10000))
testOff <- as.tibble(fread(paste0("gzip -dc ", offFiles[2]),nrow=10000))

#sample_submit <- as.tibble(fread('../input/nyc-taxi-trip-duration/sample_submission.csv'))

```

## File structure and content



Let's have an overview of the data sets using the *summary* and *glimpse* tools. First the training data:

```{r}
colnames(all_card)[1] <- 'CardType'
colnames(all_sl)[1] <- 'StopID'
colnames(scanOntrain)
scanOntrain <- inner_join(scanOntrain,all_card,by = 'CardType')
scanOntrain <- inner_join(scanOntrain,all_sl,by = 'StopID')

scanOfftrain <- inner_join(scanOfftrain,all_card,by = 'CardType')
scanOfftrain <- inner_join(scanOfftrain,all_sl,by = 'StopID')

```

```{r}

colnames(all_card)[1] <- 'CardType'
colnames(all_sl)[1] <- 'StopID'
scanOntrain <- inner_join(scanOntrain,all_card,by = 'CardType')
scanOntrain <- inner_join(scanOntrain,all_sl,by = 'StopID')

scanOfftrain <- inner_join(scanOfftrain,all_card,by = 'CardType')
scanOfftrain <- inner_join(scanOfftrain,all_sl,by = 'StopID')

summary(scanOntrain)
# join
train <- inner_join(scanOntrain,scanOfftrain,by = 'CardID')

```





```{r}

glimpse(train)

```


We find:

## Missing values



Knowing about missing values is important because they indicate how much we donâ€™t know about our data. Making inferences based on just a few cases is often unwise. In addition, many modelling procedures break down when missing values are involved and the corresponding rows will either have to be removed completely or the values need to be estimated somehow.


```{r}

sum(is.na(train))

```



## Combining train and test



In preparation for our eventual modelling analysis we combine the *train* and *test* data sets into a single one. I find it generally best not to examine the *test* data too closely, since this bears the risk of overfitting your analysis to this data. However, a few simple consistency checks between the two data sets can be of advantage.



```{r}

# combine <- bind_rows(train %>% mutate(dset = "train"), 
# 
#                      test %>% mutate(dset = "test",
# 
#                                      dropoff_datetime = NA,
# 
#                                      trip_duration = NA))
# 
# combine <- combine %>% mutate(dset = factor(dset))

```





## Reformating features



For our following analysis, we will turn the data and time from characters into *date* objects. We also recode *vendor\_id* as a factor. This makes it easier to visualise relationships that involve these features.



```{r}
# train <- train %>%
# 
#   mutate(pickup_datetime = ymd_hms(pickup_datetime),
# 
#          dropoff_datetime = ymd_hms(dropoff_datetime),
# 
#          vendor_id = factor(vendor_id),
# 
#          passenger_count = factor(passenger_count))

```





## Consistency check



It is worth checking whether the *trip\_durations* are consistent with the intervals between the *pickup\_datetime* and *dropoff\_datetime*. Presumably the former were directly computed from the latter, but you never know. Below, the *check* variable shows "TRUE" if the two intervals are not consistent:



```{r}

train %>%

  mutate(check = abs(int_length(interval(DateTime.x,DateTime.y)))) %>%

  select(check, DateTime.x, DateTime.y) %>%

  group_by(check) %>%

  count()

```



And we find that everything fits perfectly.







# Individual feature visualisations

We start with a map of melbourne and overlay a managable number of pickup coordinates to get a general overview of the locations and distances in question. For this visualisation we use the [leaflet](https://rstudio.github.io/leaflet/) package, which includes a variety of cool tools for interactive maps. In this map you can zoom and pan through the pickup locations:



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 1", out.width="100%"}

set.seed(1234)

foo <- sample_n(train, 400)

leaflet(data = foo) %>% addProviderTiles("Esri.NatGeoWorldMap") %>%

  addCircleMarkers(~ GPSLong.x.x, ~GPSLat.x.x, radius = 1,

                   color = "blue", fillOpacity = 0.3)

```



Good coverage across melbourne



```{r  fig.align = 'default', warning = FALSE, fig.cap ="Fig. 2", out.width="100%"}

train$diff_hours <- difftime(train$DateTime.x,train$DateTime.y,units = 'hours')
train$diff_hours <- as.integer(train$diff_hours)
train$diff_min <- difftime(train$DateTime.x,train$DateTime.y,units = 'mins')
train$diff_min <- as.integer(train$diff_min)

train %>%

  ggplot(aes(diff_hours)) +

  geom_histogram(fill = "red", bins = 150)

train %>%

  ggplot(aes(diff_min)) +

  geom_histogram(fill = "red", bins = 150)
```


We find:



How is there negative duration? Dataset may not be matched correctly


- Additionally, there is a strange delta-shaped peak of *trip\_duration* just before the 1e5 seconds mark and even a few way above it:



```{r warning = FALSE, out.width="100%"}

train %>%

  arrange(desc(diff_min)) %>%

  select(diff_min, DateTime.x, DateTime.y, everything()) %>%

  head(10)

```



Those records would correspond to 24-hour trips and beyond, with a maximum of almost 12 days. I know that rush hour can be bad, but those values are a little unbelievable.

Over the year, the distributions of *pickup\_datetime* and *dropoff\_datetime* look like this:



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 3", out.width="100%"}

train$Date.x <- as.Date(train$DateTime.x)
train$Date.y <- as.Date(train$DateTime.y)

p1 <- train %>%

  ggplot(aes(Date.x)) +

  geom_histogram(fill = "red", bins = 120) +

  scale_x_date(labels = date_format("%Y-%b-%d")) +
  
  labs(x = "ScanOn dates")



p2 <- train %>%

  ggplot(aes(train$Date.y)) +

  geom_histogram(fill = "blue", bins = 120) +

  scale_x_date(labels = date_format("%Y-%m-%d")) +

  labs(x = "Scanoff dates")



layout <- matrix(c(1,2),2,1,byrow=FALSE)

multiplot(p1, p2, layout=layout)

p1 <- 1; p2 <- 1

```



Fairly homogeneous, covering half a year between January and July 2016. There is an interesting drop around late January early February:



```{r eval=TRUE, fig.align = 'default', warning = FALSE, fig.cap ="Fig. 3b", out.width="100%"}

train %>%

  filter(Date.x > ymd("2015-07-01") & Date.x < ymd("2015-07-04")) %>%

  ggplot(aes(Date.x)) +

  geom_histogram(fill = "red", bins = 120) + 
  scale_x_date(labels = date_format("%Y-%b-%d")) 


```


Look at season data



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 4", fig.height=6, out.width="100%"}

most_used_station <- train %>%

  group_by(StopNameShort.x.x) %>%

  count() %>%
  
  arrange(desc(n))

most_used_station$StopNameShort.x.x <- factor(most_used_station$StopNameShort.x.x,  levels = most_used_station$StopNameShort.x.x[order(most_used_station$n,decreasing = T)])

p1 <-  ggplot(head(most_used_station,n=10),aes(StopNameShort.x.x, n, 
                                    fill = StopNameShort.x.x)) +
  geom_col() +
  
  theme(legend.position = "none",
        axis.text.x=element_text(angle = -90, hjust = 0))


most_used_suburb <- train %>%

  group_by(StopNameShort.x.x) %>%

  count() %>%
  
  arrange(desc(n))

p2 <- ggplot(head(most_used_suburb,n=10),aes(StopNameShort.x.x, n, 
                                    fill = StopNameShort.x.x)) +
  geom_col() +
  
  theme(legend.position = "none",
        axis.text.x=element_text(angle = -90, hjust = 0))

most_used_concession <- train %>%

  group_by(Concession_Type.y.x) %>%

  count() %>%
  
  arrange(desc(n))

most_used_concession$Concession_Type.y.x <- factor(most_used_concession$Concession_Type.y.x,  levels = most_used_concession$Concession_Type.y.x[order(most_used_concession$n,decreasing = T)])



p3 <- ggplot(most_used_concession,aes(Concession_Type.y.x, n, 
                                    fill = Concession_Type.y.x)) +
  geom_col() +
  
  theme(legend.position = "none",
        axis.text.x=element_text(angle = -90, hjust = 0))

p3a <- ggplot(most_used_concession,aes(Concession_Type.y.x, n, 
                                    fill = Concession_Type.y.x)) +
  geom_col() +
  
  theme(legend.position = "none",
        axis.text.x=element_text(angle = -90, hjust = 0))

p4 <- train %>%

  mutate(wday = wday(Date.x, label = TRUE)) %>%

  group_by(wday, StopNameShort.x.x) %>%

  count() %>%

  ggplot(aes(wday, n, colour = StopNameShort.x.x)) +

  geom_point(size = 4) +

  labs(x = "Day of the week", y = "Total number of Scans") +

  theme(legend.position = "none")


p5 <- train %>%

  mutate(hpick = hour(Date.x)) %>%

  group_by(hpick, StopNameShort.x.x) %>%

  count() %>%

  ggplot(aes(hpick, n, color = StopNameShort.x.x)) +

  geom_point(size = 4) +

  labs(x = "Hour of the day", y = "Total number of pickups") +

  theme(legend.position = "none")



layout <- matrix(c(1,2,3,4,5,5),3,2,byrow=TRUE)

multiplot(p1, p2, p3, p4, p5, layout=layout)

p1 <- 1; p2 <- 1; p3 <- 1; p4 <- 1; p5 <- 1

```





We find:



- There are a few trips with zero, or seven to nine passengers but they are a rare exception:



```{r}

train %>%

  group_by(CardID) %>%

  count()

```





- The vast majority of rides had only a single passenger, with two passengers being the (distant) second most popular option.



- Towards larger passenger numbers we are seeing a smooth decline through 3 to 4, until the larger crowds (and larger cars) give us another peak at 5 to 6 passengers.



- Vendor 2 has significantly more trips in this data set than vendor 1 (note the logarithmic y-axis). This is true for every day of the week.



- We find an interesting pattern with Monday being the quietest day and Friday very busy. This is the same for the two different vendors, with *vendor\_id == 2* showing significantly higher trip numbers.



- As one would intuitively expect, there is a strong dip during the early morning hours. There we also see not much difference between the two vendors. We find another dip around 4pm and then the numbers increase towards the evening.



- The *store_and_fwd_flag* values, indicating whether the trip data was sent immediately to the vendor ("N") or held in the memory of the taxi because there was no connection to the server ("Y"), show that there was almost no storing taking place (note again the logarithmic y-axis):



```{r}

train %>%

  group_by(LocalGovernmentArea.x.x) %>%

  count()

```



These numbers are equivalent to about half a percent of trips not being transmitted immediately.





The trip volume per hour of the day depends somewhat on the month and strongly on the day of the week:



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 5", fig.height=6, out.width="100%"}

p1 <- train %>%

  mutate(hpick = hour(DateTime.x),

         Month = factor(month(DateTime.x, label = TRUE))) %>%

  group_by(hpick, Month) %>%

  count() %>%

  ggplot(aes(hpick, n, color = Month)) +

  geom_line(size = 1.5) +

  labs(x = "Hour of the day", y = "count")


p2 <- train %>%

  mutate(hpick = hour(DateTime.x),

         wday = factor(wday(DateTime.x, label = TRUE))) %>%

  group_by(hpick, wday) %>%

  count() %>%

  ggplot(aes(hpick, n, color = wday)) +

  geom_line(size = 1.5) +

  labs(x = "Hour of the day", y = "count")



layout <- matrix(c(1,2),2,1,byrow=FALSE)

multiplot(p1, p2, layout=layout)

p1 <- 1; p2 <- 1

```



We find:



- January and June have fewer trips, whereas March and April are busier months. This tendency is observed for both *vendor\_ids*.



- The weekend (Sat and Sun, plus Fri to an extend) have higher trip numbers during the early morning ours but lower ones in the morning between 5 and 10, which can most likely be attributed to the contrast between NYC business days and weekend night life. In addition, trip numbers drop on a Sunday evening/night.





Finally, we will look at a simple overview visualisation of the *pickup/dropoff* latitudes and longitudes:



```{r fig.align = 'default', warning = FALSE, fig.cap ="Fig. 6", out.width="100%"}

p1 <- train %>%

  filter(GPSLong.x.x > 144 & GPSLong.x.x < 145) %>%

  ggplot(aes(GPSLong.x.x)) +

  geom_histogram(fill = "red", bins = 40)



p2 <- train %>%

  filter(GPSLong.y.x > 144 & GPSLong.y.x < 145) %>%

  ggplot(aes(GPSLong.y.x)) +

  geom_histogram(fill = "blue", bins = 40)


train$GPSLat.x.x
p3 <- train %>%

  filter(GPSLat.x.x > -38 & GPSLat.x.x < 37) %>%

  ggplot(aes(GPSLat.x.x)) +

  geom_histogram(fill = "red", bins = 40)

p4 <- train %>%

  filter(GPSLat.y.x > 144 & GPSLat.y.x < 145) %>%

  ggplot(aes(GPSLat.y.x)) +

  geom_histogram(fill = "blue", bins = 40)



layout <- matrix(c(1,2,3,4),2,2,byrow=FALSE)

multiplot(p1, p2, p3, p4, layout=layout)

p1 <- 1; p2 <- 1; p3 <- 1; p4 <- 1

```

